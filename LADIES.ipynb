{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import scipy\n",
    "import multiprocessing as mp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parser = argparse.ArgumentParser(description='Training GCN on Cora/CiteSeer/PubMed/Reddit Datasets')\n",
    "\n",
    "'''\n",
    "    Dataset arguments\n",
    "'''\n",
    "parser.add_argument('--dataset', type=str, default='cora',\n",
    "                    help='Dataset name: cora/citeseer/pubmed/Reddit')\n",
    "parser.add_argument('--nhid', type=int, default=256,\n",
    "                    help='Hidden state dimension')\n",
    "parser.add_argument('--epoch_num', type=int, default= 100,\n",
    "                    help='Number of Epoch')\n",
    "parser.add_argument('--pool_num', type=int, default= 10,\n",
    "                    help='Number of Pool')\n",
    "parser.add_argument('--batch_num', type=int, default= 10,\n",
    "                    help='Maximum Batch Number')\n",
    "parser.add_argument('--batch_size', type=int, default=512,\n",
    "                    help='size of output node in a batch')\n",
    "parser.add_argument('--n_layers', type=int, default=5,\n",
    "                    help='Number of GCN layers')\n",
    "parser.add_argument('--n_iters', type=int, default=1,\n",
    "                    help='Number of iteration to run on a batch')\n",
    "parser.add_argument('--n_stops', type=int, default=200,\n",
    "                    help='Stop after number of batches that f1 dont increase')\n",
    "parser.add_argument('--samp_num', type=int, default=64,\n",
    "                    help='Number of sampled nodes per layer')\n",
    "parser.add_argument('--sample_method', type=str, default='ladies',\n",
    "                    help='Sampled Algorithms: ladies/fastgcn/full')\n",
    "parser.add_argument('--cuda', type=int, default=-1,\n",
    "                    help='Avaiable GPU ID')\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args(\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, n_in, n_out, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.n_in  = n_in\n",
    "        self.n_out = n_out\n",
    "        self.linear = nn.Linear(n_in,  n_out)\n",
    "    def forward(self, x, adj):\n",
    "        out = self.linear(x)\n",
    "        return F.elu(torch.spmm(adj, out))\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, layers, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.nhid = nhid\n",
    "        self.gcs = nn.ModuleList()\n",
    "        self.gcs.append(GraphConvolution(nfeat,  nhid))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        for i in range(layers-1):\n",
    "            self.gcs.append(GraphConvolution(nhid,  nhid))\n",
    "    def forward(self, x, adjs):\n",
    "        '''\n",
    "            The difference here with the original GCN implementation is that\n",
    "            we will receive different adjacency matrix for different layer.\n",
    "        '''\n",
    "        for idx in range(len(self.gcs)):\n",
    "            x = self.dropout(self.gcs[idx](x, adjs[idx]))\n",
    "        return x\n",
    "\n",
    "class SuGCN(nn.Module):\n",
    "    def __init__(self, encoder, num_classes, dropout, inp):\n",
    "        super(SuGCN, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear  = nn.Linear(self.encoder.nhid, num_classes)\n",
    "    def forward(self, feat, adjs):\n",
    "        x = self.encoder(feat, adjs)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def fastgcn_sampler(seed, batch_nodes, samp_num_list, num_nodes, lap_matrix, depth):\n",
    "    '''\n",
    "        FastGCN_Sampler: Sample a fixed number of nodes per layer. The sampling probability (importance)\n",
    "                         is pre-computed based on the global degree (lap_matrix)\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    previous_nodes = batch_nodes\n",
    "    adjs  = []\n",
    "    #     pre-compute the sampling probability (importance) based on the global degree (lap_matrix)\n",
    "    pi = np.array(np.sum(lap_matrix.multiply(lap_matrix), axis=0))[0]\n",
    "    p = pi / np.sum(pi)\n",
    "    '''\n",
    "        Sample nodes from top to bottom, based on the pre-computed probability. Then reconstruct the adjacency matrix.\n",
    "    '''\n",
    "    for d in range(depth):\n",
    "        #     row-select the lap_matrix (U) by previously sampled nodes\n",
    "        U = lap_matrix[previous_nodes , :]\n",
    "        #     sample the next layer's nodes based on the pre-computed probability (p).\n",
    "        s_num = np.min([np.sum(p > 0), samp_num_list[d]])\n",
    "        after_nodes = np.random.choice(num_nodes, s_num, p = p, replace = False)\n",
    "        #     col-select the lap_matrix (U), and then devided by the sampled probability for \n",
    "        #     unbiased-sampling. Finally, conduct row-normalization to avoid value explosion.         \n",
    "        adj = row_norm(U[: , after_nodes].multiply(1/p[after_nodes]))\n",
    "        #     Turn the sampled adjacency matrix into a sparse matrix. If implemented by PyG\n",
    "        #     This sparse matrix can also provide index and value.\n",
    "        adjs += [sparse_mx_to_torch_sparse_tensor(row_normalize(adj))]\n",
    "        #     Turn the sampled nodes as previous_nodes, recursively conduct sampling.\n",
    "        previous_nodes = after_nodes\n",
    "    #     Reverse the sampled probability from bottom to top. Only require input how the lastly sampled nodes.\n",
    "    adjs.reverse()\n",
    "    return adjs, previous_nodes, batch_nodes\n",
    "\n",
    "def ladies_sampler(seed, batch_nodes, samp_num_list, num_nodes, lap_matrix, depth):\n",
    "    '''\n",
    "        LADIES_Sampler: Sample a fixed number of nodes per layer. The sampling probability (importance)\n",
    "                         is computed adaptively according to the nodes sampled in the upper layer.\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    previous_nodes = batch_nodes\n",
    "    adjs  = []\n",
    "    '''\n",
    "        Sample nodes from top to bottom, based on the probability computed adaptively (layer-dependent).\n",
    "    '''\n",
    "    for d in range(depth):\n",
    "        #     row-select the lap_matrix (U) by previously sampled nodes\n",
    "        U = lap_matrix[previous_nodes , :]\n",
    "        #     Only use the upper layer's neighborhood to calculate the probability.\n",
    "        pi = np.array(np.sum(U.multiply(U), axis=0))[0]\n",
    "        p = pi / np.sum(pi)\n",
    "        s_num = np.min([np.sum(p > 0), samp_num_list[d]])\n",
    "        #     sample the next layer's nodes based on the adaptively probability (p).\n",
    "        after_nodes = np.random.choice(num_nodes, s_num, p = p, replace = False)\n",
    "        #     Add output nodes for self-loop\n",
    "        after_nodes = np.unique(np.concatenate((after_nodes, batch_nodes)))\n",
    "        #     col-select the lap_matrix (U), and then devided by the sampled probability for \n",
    "        #     unbiased-sampling. Finally, conduct row-normalization to avoid value explosion.      \n",
    "        adj = U[: , after_nodes].multiply(1/p[after_nodes])\n",
    "        adjs += [sparse_mx_to_torch_sparse_tensor(row_normalize(adj))]\n",
    "        #     Turn the sampled nodes as previous_nodes, recursively conduct sampling.\n",
    "        previous_nodes = after_nodes\n",
    "    #     Reverse the sampled probability from bottom to top. Only require input how the lastly sampled nodes.\n",
    "    adjs.reverse()\n",
    "    return adjs, previous_nodes, batch_nodes\n",
    "\n",
    "def default_sampler(seed, batch_nodes, samp_num_list, num_nodes, lap_matrix, depth):\n",
    "    mx = sparse_mx_to_torch_sparse_tensor(lap_matrix)\n",
    "    return [mx for i in range(depth)], np.arange(num_nodes), batch_nodes\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def prepare_data(pool, sampler, process_ids, train_nodes, valid_nodes, samp_num_list, num_nodes, lap_matrix, depth):\n",
    "    jobs = []\n",
    "    for _ in process_ids:\n",
    "        idx = torch.randperm(len(train_nodes))[:args.batch_size]\n",
    "        batch_nodes = train_nodes[idx]\n",
    "        p = pool.apply_async(sampler, args=(np.random.randint(2**32 - 1), batch_nodes, samp_num_list, num_nodes, lap_matrix, depth))\n",
    "        jobs.append(p)\n",
    "    idx = torch.randperm(len(valid_nodes))[:args.batch_size]\n",
    "    batch_nodes = valid_nodes[idx]\n",
    "    p = pool.apply_async(sampler, args=(np.random.randint(2**32 - 1), batch_nodes, samp_num_list * 20, num_nodes, lap_matrix, depth))\n",
    "    jobs.append(p)\n",
    "    return jobs\n",
    "\n",
    "def package_mxl(mxl, device):\n",
    "    return [torch.sparse.FloatTensor(mx[0], mx[1], mx[2]).to(device) for mx in mxl]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if args.cuda != -1:\n",
    "    device = torch.device(\"cuda:\" + str(args.cuda))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(args.dataset, args.sample_method)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edges, labels, feat_data, num_classes, train_nodes, valid_nodes, test_nodes = load_data(args.dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Dataset: Cora\n",
    "# nodes = 2708\n",
    "# edges = 10858\n",
    "# feature set : 2708, 1433\n",
    "# num_classes : 7\n",
    "# dataset split :\n",
    "#   train: 140\n",
    "#   val: 500\n",
    "#   test: 1000\n",
    "\n",
    "## Dataset: citeseer\n",
    "# nodes = 3327\n",
    "# edges = 9464\n",
    "# feature set : 3327, 3703\n",
    "# num_classes : 6\n",
    "# dataset split :\n",
    "#   train: 120\n",
    "#   val: 500\n",
    "#   test: 1000\n",
    "\n",
    "## Dataset: PubMed\n",
    "# nodes = 19717\n",
    "# edges = 88676\n",
    "# feature set : 19717, 500\n",
    "# num_classes : 3\n",
    "# dataset split :\n",
    "#   train: 60\n",
    "#   val: 500\n",
    "#   test: 1000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "adj_matrix = get_adj(edges, feat_data.shape[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "adj_matrix.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lap_matrix = row_normalize(adj_matrix + sp.eye(adj_matrix.shape[0]))\n",
    "if type(feat_data) == scipy.sparse.lil.lil_matrix:\n",
    "    feat_data = torch.FloatTensor(feat_data.todense()).to(device)\n",
    "else:\n",
    "    feat_data = torch.FloatTensor(feat_data).to(device)\n",
    "labels = torch.LongTensor(labels).to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if args.sample_method == 'ladies':\n",
    "    sampler = ladies_sampler\n",
    "elif args.sample_method == 'fastgcn':\n",
    "    sampler = fastgcn_sampler\n",
    "elif args.sample_method == 'full':\n",
    "    sampler = default_sampler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "process_ids = np.arange(args.batch_num)\n",
    "samp_num_list = np.array([args.samp_num, args.samp_num, args.samp_num, args.samp_num, args.samp_num])\n",
    "\n",
    "pool = mp.Pool(args.pool_num)\n",
    "jobs = prepare_data(pool, sampler, process_ids, train_nodes, valid_nodes, samp_num_list, len(feat_data), lap_matrix, args.n_layers)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_res = {}\n",
    "for oiter in range(5):\n",
    "    encoder = GCN(nfeat = feat_data.shape[1], nhid=args.nhid, layers=args.n_layers, dropout = 0.2).to(device)\n",
    "    susage  = SuGCN(encoder = encoder, num_classes=num_classes, dropout=0.5, inp = feat_data.shape[1])\n",
    "    susage.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(filter(lambda p : p.requires_grad, susage.parameters()))\n",
    "    best_val = 0\n",
    "    best_tst = -1\n",
    "    cnt = 0\n",
    "    times = []\n",
    "    all_res[oiter] = {'train': [], 'test': []}\n",
    "    print('-' * 10)\n",
    "    for epoch in np.arange(args.epoch_num):\n",
    "        susage.train()\n",
    "        train_losses = []\n",
    "        train_data = [job.get() for job in jobs[:-1]]\n",
    "        valid_data = jobs[-1].get()\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        pool = mp.Pool(args.pool_num)\n",
    "        '''\n",
    "            Use CPU-GPU cooperation to reduce the overhead for sampling. (conduct sampling while training)\n",
    "        '''\n",
    "        jobs = prepare_data(pool, sampler, process_ids, train_nodes, valid_nodes, samp_num_list, len(feat_data), lap_matrix, args.n_layers)\n",
    "        for _iter in range(args.n_iters):\n",
    "            for adjs, input_nodes, output_nodes in train_data:    \n",
    "                adjs = package_mxl(adjs, device)\n",
    "                optimizer.zero_grad()\n",
    "                t1 = time.time()\n",
    "                susage.train()\n",
    "                output = susage.forward(feat_data[input_nodes], adjs)\n",
    "                if args.sample_method == 'full':\n",
    "                    output = output[output_nodes]\n",
    "                loss_train = F.cross_entropy(output, labels[output_nodes])\n",
    "                loss_train.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(susage.parameters(), 0.2)\n",
    "                optimizer.step()\n",
    "                times += [time.time() - t1]\n",
    "                train_losses += [loss_train.detach().tolist()]\n",
    "                del loss_train\n",
    "        susage.eval()\n",
    "        adjs, input_nodes, output_nodes = valid_data\n",
    "        adjs = package_mxl(adjs, device)\n",
    "        output = susage.forward(feat_data[input_nodes], adjs)\n",
    "        if args.sample_method == 'full':\n",
    "            output = output[output_nodes]\n",
    "        loss_valid = F.cross_entropy(output, labels[output_nodes]).detach().tolist()\n",
    "        valid_f1 = f1_score(output.argmax(dim=1).cpu(), labels[output_nodes].cpu(), average='micro')\n",
    "        print((\"Epoch: %d (%.1fs) Train Loss: %.2f    Valid Loss: %.2f Valid F1: %.3f\") %                   (epoch, np.sum(times), np.average(train_losses), loss_valid, valid_f1))\n",
    "        all_res[oiter]['train'].append({'epoch': epoch, 'time': np.sum(times), 'train_loss': np.average(train_losses), 'val_losses': loss_valid, 'val_f1': valid_f1})\n",
    "        if valid_f1 > best_val + 1e-2:\n",
    "            best_val = valid_f1\n",
    "            torch.save(susage, './models/best_model.pt')\n",
    "            cnt = 0\n",
    "        else:\n",
    "            cnt += 1\n",
    "        if cnt == args.n_stops // args.batch_num:\n",
    "            break\n",
    "    best_model = torch.load('./models/best_model.pt')\n",
    "    best_model.eval()\n",
    "    test_f1s = []\n",
    "    \n",
    "    '''\n",
    "    If using batch sampling for inference:\n",
    "    '''\n",
    "    #     for b in np.arange(len(test_nodes) // args.batch_size):\n",
    "    #         batch_nodes = test_nodes[b * args.batch_size : (b+1) * args.batch_size]\n",
    "    #         adjs, input_nodes, output_nodes = sampler(np.random.randint(2**32 - 1), batch_nodes,\n",
    "    #                                     samp_num_list * 20, len(feat_data), lap_matrix, args.n_layers)\n",
    "    #         adjs = package_mxl(adjs, device)\n",
    "    #         output = best_model.forward(feat_data[input_nodes], adjs)[output_nodes]\n",
    "    #         test_f1 = f1_score(output.argmax(dim=1).cpu(), labels[output_nodes].cpu(), average='micro')\n",
    "    #         test_f1s += [test_f1]\n",
    "    \n",
    "    '''\n",
    "    If using full-batch inference:\n",
    "    '''\n",
    "    batch_nodes = test_nodes\n",
    "    adjs, input_nodes, output_nodes = default_sampler(np.random.randint(2**32 - 1), batch_nodes,\n",
    "                                    samp_num_list * 20, len(feat_data), lap_matrix, args.n_layers)\n",
    "    adjs = package_mxl(adjs, device)\n",
    "    output = best_model.forward(feat_data[input_nodes], adjs)[output_nodes]\n",
    "    test_f1s = [f1_score(output.argmax(dim=1).cpu(), labels[output_nodes].cpu(), average='micro')]\n",
    "    all_res[oiter]['test'] = test_f1s\n",
    "    print('Iteration: %d, Test F1: %.3f' % (oiter, np.average(test_f1s)))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in all_res:\n",
    "    df_temp = pd.DataFrame(all_res[i]['train'])\n",
    "    plt.plot(df_temp['epoch'], df_temp['train_loss'], label=str(i) + '_train')\n",
    "    plt.plot(df_temp['epoch'], df_temp['val_losses'], label=str(i) + '_val')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('train loss')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "for i in all_res:\n",
    "    df_temp = pd.DataFrame(all_res[i]['train'])\n",
    "    plt.plot(df_temp['epoch'], df_temp['val_f1'], label=str(i))\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('val F1')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "t = []\n",
    "for i in all_res:\n",
    "    t.append(np.average(all_res[i]['test']))\n",
    "plt.plot(range(len(t)), t)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('test F1')\n",
    "\n",
    "print ('average test F1:', np.average(t))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('lts': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "metadata": {
   "interpreter": {
    "hash": "589776dd14f65bb17e24bc135d46bd968269cccd64c5cd0e2744d5b7f85103ad"
   }
  },
  "orig_nbformat": 2,
  "interpreter": {
   "hash": "0aa9aa112e52f62a237da6cfdc39bb5f0ca0f0e0f203cd8d7c23fb0db2387531"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}