{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import torch_geometric.utils as tg\n",
    "import networkx as nx\n",
    "import torch_geometric\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import scipy\n",
    "import multiprocessing as mp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parser = argparse.ArgumentParser(description='Training GCN on Cora/CiteSeer/PubMed/Reddit Datasets')\n",
    "\n",
    "'''\n",
    "    Dataset arguments\n",
    "'''\n",
    "parser.add_argument('--dataset', type=str, default='cora',\n",
    "                    help='Dataset name: cora/citeseer/pubmed/Reddit')\n",
    "parser.add_argument('--nhid', type=int, default=256,\n",
    "                    help='Hidden state dimension')\n",
    "parser.add_argument('--epoch_num', type=int, default= 100,\n",
    "                    help='Number of Epoch')\n",
    "# parser.add_argument('--epoch_num', type=int, default= 1,\n",
    "                    # help='Number of Epoch')\n",
    "parser.add_argument('--pool_num', type=int, default= 10,\n",
    "                    help='Number of Pool')\n",
    "# parser.add_argument('--pool_num', type=int, default= 1,\n",
    "#                     help='Number of Pool')\n",
    "parser.add_argument('--batch_num', type=int, default= 10,\n",
    "                    help='Maximum Batch Number')\n",
    "# parser.add_argument('--batch_num', type=int, default= 1,\n",
    "#                     help='Maximum Batch Number')\n",
    "parser.add_argument('--batch_size', type=int, default=512,\n",
    "                    help='size of output node in a batch')\n",
    "parser.add_argument('--n_layers', type=int, default=5,\n",
    "                    help='Number of GCN layers')\n",
    "parser.add_argument('--n_iters', type=int, default=1,\n",
    "                    help='Number of iteration to run on a batch')\n",
    "parser.add_argument('--n_stops', type=int, default=200,\n",
    "                    help='Stop after number of batches that f1 dont increase')\n",
    "parser.add_argument('--samp_num', type=int, default=64,\n",
    "                    help='Number of sampled nodes per layer')\n",
    "parser.add_argument('--sample_method', type=str, default='lts',\n",
    "                    help='Sampled Algorithms: ladies/fastgcn/full/lts')\n",
    "parser.add_argument('--cuda', type=int, default=-1,\n",
    "                    help='Avaiable GPU ID')\n",
    "parser.add_argument('--filename', type = str, default='untitled.txt', help = 'Output file name')\n",
    "\n",
    "\n",
    "args = parser.parse_args(\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, n_in, n_out, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.n_in  = n_in\n",
    "        self.n_out = n_out\n",
    "        self.linear = nn.Linear(n_in,  n_out)\n",
    "    def forward(self, x, adj):\n",
    "        out = self.linear(x)\n",
    "        return F.elu(torch.spmm(adj, out))\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, layers, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.nhid = nhid\n",
    "        self.gcs = nn.ModuleList()\n",
    "        self.gcs.append(GraphConvolution(nfeat,  nhid))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        for i in range(layers-1):\n",
    "            self.gcs.append(GraphConvolution(nhid,  nhid))\n",
    "    def forward(self, x, adjs):\n",
    "        '''\n",
    "            The difference here with the original GCN implementation is that\n",
    "            we will receive different adjacency matrix for different layer.\n",
    "        '''\n",
    "        for idx in range(len(self.gcs)):\n",
    "            x = self.dropout(self.gcs[idx](x, adjs[idx]))\n",
    "        return x\n",
    "\n",
    "class SuGCN(nn.Module):\n",
    "    def __init__(self, encoder, num_classes, dropout, inp):\n",
    "        super(SuGCN, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear  = nn.Linear(self.encoder.nhid, num_classes)\n",
    "    def forward(self, feat, adjs):\n",
    "        x = self.encoder(feat, adjs)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def fastgcn_sampler(seed, batch_nodes, samp_num_list, num_nodes, lap_matrix, depth):\n",
    "    '''\n",
    "        FastGCN_Sampler: Sample a fixed number of nodes per layer. The sampling probability (importance)\n",
    "                         is pre-computed based on the global degree (lap_matrix)\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    previous_nodes = batch_nodes\n",
    "    adjs  = []\n",
    "    #     pre-compute the sampling probability (importance) based on the global degree (lap_matrix)\n",
    "    pi = np.array(np.sum(lap_matrix.multiply(lap_matrix), axis=0))[0]\n",
    "    p = pi / np.sum(pi)\n",
    "    '''\n",
    "        Sample nodes from top to bottom, based on the pre-computed probability. Then reconstruct the adjacency matrix.\n",
    "    '''\n",
    "    for d in range(depth):\n",
    "        #     row-select the lap_matrix (U) by previously sampled nodes\n",
    "        U = lap_matrix[previous_nodes , :]\n",
    "        #     sample the next layer's nodes based on the pre-computed probability (p).\n",
    "        s_num = np.min([np.sum(p > 0), samp_num_list[d]])\n",
    "        after_nodes = np.random.choice(num_nodes, s_num, p = p, replace = False)\n",
    "        #     col-select the lap_matrix (U), and then devided by the sampled probability for \n",
    "        #     unbiased-sampling. Finally, conduct row-normalization to avoid value explosion.         \n",
    "        adj = row_norm(U[: , after_nodes].multiply(1/p[after_nodes]))\n",
    "        #     Turn the sampled adjacency matrix into a sparse matrix. If implemented by PyG\n",
    "        #     This sparse matrix can also provide index and value.\n",
    "        adjs += [sparse_mx_to_torch_sparse_tensor(row_normalize(adj))]\n",
    "        #     Turn the sampled nodes as previous_nodes, recursively conduct sampling.\n",
    "        previous_nodes = after_nodes\n",
    "    #     Reverse the sampled probability from bottom to top. Only require input how the lastly sampled nodes.\n",
    "    adjs.reverse()\n",
    "    return adjs, previous_nodes, batch_nodes\n",
    "\n",
    "def ladies_sampler(seed, batch_nodes, samp_num_list, num_nodes, lap_matrix, depth, prob_norm=None):\n",
    "    '''\n",
    "        LADIES_Sampler: Sample a fixed number of nodes per layer. The sampling probability (importance)\n",
    "                         is computed adaptively according to the nodes sampled in the upper layer.\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    previous_nodes = batch_nodes\n",
    "    adjs  = []\n",
    "    '''\n",
    "        Sample nodes from top to bottom, based on the probability computed adaptively (layer-dependent).\n",
    "    '''\n",
    "    for d in range(depth):\n",
    "        #     row-select the lap_matrix (U) by previously sampled nodes\n",
    "        U = lap_matrix[previous_nodes , :]\n",
    "        #     Only use the upper layer's neighborhood to calculate the probability.\n",
    "        pi = np.array(np.sum(U.multiply(U), axis=0))[0]\n",
    "        p = pi / np.sum(pi)\n",
    "        s_num = np.min([np.sum(p > 0), samp_num_list[d]])\n",
    "        #     sample the next layer's nodes based on the adaptively probability (p).\n",
    "        after_nodes = np.random.choice(num_nodes, s_num, p = p, replace = False)\n",
    "        #     Add output nodes for self-loop\n",
    "        after_nodes = np.unique(np.concatenate((after_nodes, batch_nodes)))\n",
    "        #     col-select the lap_matrix (U), and then devided by the sampled probability for \n",
    "        #     unbiased-sampling. Finally, conduct row-normalization to avoid value explosion.      \n",
    "        adj = U[: , after_nodes].multiply(1/p[after_nodes])\n",
    "        adjs += [sparse_mx_to_torch_sparse_tensor(row_normalize(adj))]\n",
    "        #     Turn the sampled nodes as previous_nodes, recursively conduct sampling.\n",
    "        previous_nodes = after_nodes\n",
    "    #     Reverse the sampled probability from bottom to top. Only require input how the lastly sampled nodes.\n",
    "    adjs.reverse()\n",
    "    return adjs, previous_nodes, batch_nodes\n",
    "\n",
    "def lts_sampler(seed, batch_nodes, samp_num_list, num_nodes, lap_matrix, depth, prob_norm):\n",
    "    '''\n",
    "        LTS_Sampler: Sample a fixed number of nodes per layer. The sampling probability (importance)\n",
    "                         is computed adaptively according to the nodes sampled in the upper layer.\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    previous_nodes = batch_nodes\n",
    "    adjs  = []\n",
    "    '''\n",
    "        Sample nodes from top to bottom, based on the probability computed adaptively (layer-dependent).\n",
    "    '''\n",
    "    for d in range(depth):\n",
    "        \n",
    "        #     row-select the lap_matrix (U) by previously sampled nodes\n",
    "        #U = lap_matrix[previous_nodes , :]\n",
    "        #    Only use the upper layer's neighborhood to calculate the probability.\n",
    "        #pi = np.array(np.sum(U.multiply(U), axis=0))[0]\n",
    "        #p = pi / np.sum(pi)\n",
    "        #--------------------------------------------------------------------------------\n",
    "        ''' finding the neighbors. We will find an array of size number of nodes in the graph\n",
    "            in which only neighors of previous can have non zero probability. Non neighbors \n",
    "            should have zero probability. \n",
    "        '''\n",
    "        temp_lap = lap_matrix\n",
    "        # selecting only rows of nodes previous nodes to find neighbors of only those nodes.\n",
    "        temp = temp_lap[previous_nodes, :]\n",
    "        # Summing will give us non zero entries in respective columns if they are neighbors\n",
    "        # ravel will convert it to 1D array\n",
    "        nbrs = np.ravel(np.sum(temp, axis = 0))\n",
    "        # entries in array/list can be lesser/greater than 1, making them 1 if node is neighbor\n",
    "        # 0 elsewise\n",
    "        for i in range(len(nbrs)):\n",
    "            if nbrs[i]>0:\n",
    "                nbrs[i] = 1\n",
    "        # Multiply elementwise will give us array with neigbors with probability, 0 for non neighbors\n",
    "        pi = np.multiply(nbrs, prob_norm)    \n",
    "        # normalization\n",
    "        p = pi / np.linalg.norm(pi, ord = 1)\n",
    "\n",
    "        #--------------------------------------------------------------------------------\n",
    "        s_num = np.min([np.sum(p > 0), samp_num_list[d]])\n",
    "        #     sample the next layer's nodes based on the adaptively probability (p).\n",
    "        after_nodes = np.random.choice(num_nodes, s_num, p = p, replace = False)\n",
    "        #     Add output nodes for self-loop\n",
    "        after_nodes = np.unique(np.concatenate((after_nodes, batch_nodes)))\n",
    "        #     col-select the lap_matrix (U), and then devided by the sampled probability for \n",
    "        #     unbiased-sampling. Finally, conduct row-normalization to avoid value explosion.      \n",
    "        adj = temp[: , after_nodes].multiply(1/p[after_nodes])\n",
    "        new_adj = sparse_mx_to_torch_sparse_tensor(row_normalize(adj))\n",
    "        adjs += [new_adj]\n",
    "        \n",
    "        #     Turn the sampled nodes as previous_nodes, recursively conduct sampling.\n",
    "        previous_nodes = after_nodes\n",
    "    #     Reverse the sampled probability from bottom to top. Only require input how the lastly sampled nodes.\n",
    "    adjs.reverse()\n",
    "    return adjs, previous_nodes, batch_nodes\n",
    "\n",
    "def default_sampler(seed, batch_nodes, samp_num_list, num_nodes, lap_matrix, depth):\n",
    "    mx = sparse_mx_to_torch_sparse_tensor(lap_matrix)\n",
    "    return [mx for i in range(depth)], np.arange(num_nodes), batch_nodes\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def prepare_data(pool, sampler, process_ids, train_nodes, valid_nodes, samp_num_list, num_nodes, lap_matrix, depth, prob_norm):\n",
    "    jobs = []\n",
    "    for _ in process_ids:\n",
    "        idx = torch.randperm(len(train_nodes))[:args.batch_size]\n",
    "        batch_nodes = train_nodes[idx]\n",
    "        p = pool.apply_async(sampler, args=(np.random.randint(2**32 - 1), batch_nodes, samp_num_list, num_nodes, lap_matrix, depth, prob_norm))\n",
    "        jobs.append(p)\n",
    "    idx = torch.randperm(len(valid_nodes))[:args.batch_size]\n",
    "    batch_nodes = valid_nodes[idx]\n",
    "    p = pool.apply_async(sampler, args=(np.random.randint(2**32 - 1), batch_nodes, samp_num_list * 20, num_nodes, lap_matrix, depth, prob_norm))\n",
    "    jobs.append(p)\n",
    "    return jobs\n",
    "\n",
    "def package_mxl(mxl, device):\n",
    "    return [torch.sparse.FloatTensor(mx[0], mx[1], mx[2]).to(device) for mx in mxl]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f = open(args.filename, 'w')\n",
    "\n",
    "if args.cuda != -1:\n",
    "    device = torch.device(\"cuda:\" + str(args.cuda))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(args.dataset, args.sample_method)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edges, labels, feat_data, num_classes, train_nodes, valid_nodes, test_nodes = load_data(args.dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "adj_matrix = get_adj(edges, feat_data.shape[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lap_matrix = row_normalize(adj_matrix + sp.eye(adj_matrix.shape[0]))\n",
    "if type(feat_data) == scipy.sparse.lil.lil_matrix:\n",
    "    feat_data = torch.FloatTensor(feat_data.todense()).to(device) \n",
    "else:\n",
    "    feat_data = torch.FloatTensor(feat_data).to(device)\n",
    "labels    = torch.LongTensor(labels).to(device) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if args.sample_method == 'ladies':\n",
    "    sampler = ladies_sampler\n",
    "elif args.sample_method == 'lts':\n",
    "    sampler = lts_sampler\n",
    "elif args.sample_method == 'fastgcn':\n",
    "    sampler = fastgcn_sampler\n",
    "elif args.sample_method == 'full':\n",
    "    sampler = default_sampler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#------------------------------------------------------\n",
    "# Modifications to original code\n",
    "\n",
    "# converting to torch graph\n",
    "adj_torch = torch_geometric.data.Data(edge_index = torch.tensor([edges[:,0], edges[:,1]]))\n",
    "# converting to networkx graph\n",
    "adj_nx = tg.to_networkx(adj_torch)\n",
    "adj_nx.num_nodes = feat_data.shape[0]\n",
    "\n",
    "# calculating centralities\n",
    "eigen_cen = list(nx.eigenvector_centrality(adj_nx).values())\n",
    "bet_cen = list(nx.betweenness_centrality(adj_nx).values())\n",
    "clos_cen = list(nx.closeness_centrality(adj_nx).values())\n",
    "deg_cen = list(nx.degree_centrality(adj_nx).values())\n",
    "cen = [eigen_cen, bet_cen, clos_cen, deg_cen]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "feature_set = np.array(cen)\n",
    "i = 0\n",
    "def optimization_function(x):\n",
    "    global feature_set\n",
    "    global i\n",
    "    print('iteration number', i)\n",
    "    i += 1\n",
    "\n",
    "    ## should we normalise here?\n",
    "    # x = x / np.sum(x)\n",
    "    params = np.zeros(feature_set.shape) + np.array(x).reshape(-1,1)\n",
    "\n",
    "    prob_unnorm = np.sum(params * feature_set, axis = 0)\n",
    "    ## take norm or take softmax\n",
    "    prob_norm = np.exp(prob_unnorm) / sum(np.exp(prob_unnorm))\n",
    "    # prob_norm = prob_unnorm / np.linalg.norm( prob_unnorm, ord = 1 )\n",
    "\n",
    "    print('-------------------------------------------------------------------')\n",
    "    print('params = ', x)\n",
    "\n",
    "    process_ids = np.arange(args.batch_num)\n",
    "    samp_num_list = np.array([args.samp_num, args.samp_num, args.samp_num, args.samp_num, args.samp_num])\n",
    "\n",
    "    pool = mp.Pool(args.pool_num)\n",
    "    jobs = prepare_data(pool, sampler, process_ids, train_nodes, valid_nodes, samp_num_list, len(feat_data), lap_matrix, args.n_layers, prob_norm)\n",
    "    all_res = []\n",
    "    for oiter in range(1):\n",
    "        encoder = GCN(nfeat = feat_data.shape[1], nhid=args.nhid, layers=args.n_layers, dropout = 0.2).to(device)\n",
    "        susage  = SuGCN(encoder = encoder, num_classes=num_classes, dropout=0.5, inp = feat_data.shape[1])\n",
    "        susage.to(device)\n",
    "        \n",
    "        optimizer = optim.Adam(filter(lambda p : p.requires_grad, susage.parameters()))\n",
    "        best_val = 0\n",
    "        best_tst = -1\n",
    "        cnt = 0\n",
    "        times = []\n",
    "        res   = []\n",
    "        print('-' * 10)\n",
    "        for epoch in np.arange(args.epoch_num):\n",
    "            susage.train()\n",
    "            train_losses = []\n",
    "            train_data = [job.get() for job in jobs[:-1]]\n",
    "            valid_data = jobs[-1].get()\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            pool = mp.Pool(args.pool_num)\n",
    "            '''\n",
    "                Use CPU-GPU cooperation to reduce the overhead for sampling. (conduct sampling while training)\n",
    "            '''\n",
    "            jobs = prepare_data(pool, sampler, process_ids, train_nodes, valid_nodes, samp_num_list, len(feat_data), lap_matrix, args.n_layers, prob_norm)\n",
    "            for _iter in range(args.n_iters):\n",
    "                for adjs, input_nodes, output_nodes in train_data:    \n",
    "                    adjs = package_mxl(adjs, device)\n",
    "                    optimizer.zero_grad()\n",
    "                    t1 = time.time()\n",
    "                    susage.train()\n",
    "                    output = susage.forward(feat_data[input_nodes], adjs)\n",
    "                \n",
    "                    if args.sample_method == 'full':\n",
    "                        output = output[output_nodes]\n",
    "                    loss_train = F.cross_entropy(output, labels[output_nodes])\n",
    "                \n",
    "                    loss_train.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(susage.parameters(), 0.2)\n",
    "                    optimizer.step()\n",
    "                    times += [time.time() - t1]\n",
    "                    train_losses += [loss_train.detach().tolist()]\n",
    "                    del loss_train\n",
    "            susage.eval()\n",
    "            adjs, input_nodes, output_nodes = valid_data\n",
    "            adjs = package_mxl(adjs, device)\n",
    "            output = susage.forward(feat_data[input_nodes], adjs)\n",
    "            if args.sample_method == 'full':\n",
    "                output = output[output_nodes]\n",
    "            loss_valid = F.cross_entropy(output, labels[output_nodes]).detach().tolist()\n",
    "            valid_f1 = f1_score(output.argmax(dim=1).cpu(), labels[output_nodes].cpu(), average='micro')\n",
    "            print((\"Epoch: %d (%.1fs) Train Loss: %.2f    Valid Loss: %.2f Valid F1: %.3f\") % (epoch, np.sum(times), np.average(train_losses), loss_valid, valid_f1))\n",
    "\n",
    "            if valid_f1 > best_val + 1e-2:\n",
    "                best_val = valid_f1\n",
    "                torch.save(susage, './models/best_model.pt')\n",
    "                cnt = 0\n",
    "            else:\n",
    "                cnt += 1\n",
    "            if cnt == args.n_stops // args.batch_num:\n",
    "                break\n",
    "        \n",
    "    return (1-best_val)\n",
    "\n",
    "    '''\n",
    "            If using batch sampling for inference:\n",
    "    '''\n",
    "        #     for b in np.arange(len(test_nodes) // args.batch_size):\n",
    "        #         batch_nodes = test_nodes[b * args.batch_size : (b+1) * args.batch_size]\n",
    "        #         adjs, input_nodes, output_nodes = sampler(np.random.randint(2**32 - 1), batch_nodes,\n",
    "        #                                     samp_num_list * 20, len(feat_data), lap_matrix, args.n_layers)\n",
    "        #         adjs = package_mxl(adjs, device)\n",
    "        #         output = best_model.forward(feat_data[input_nodes], adjs)[output_nodes]\n",
    "        #         test_f1 = f1_score(output.argmax(dim=1).cpu(), labels[output_nodes].cpu(), average='micro')\n",
    "        #         test_f1s += [test_f1]\n",
    "        \n",
    "    '''\n",
    "            If using full-batch inference:\n",
    "    \n",
    "    '''\n",
    "#------------------------------------------------------\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.plots import plot_convergence\n",
    "\n",
    "start = time.time()\n",
    "res = gp_minimize(optimization_function, [(0.0, 1.0)] * feature_set.shape[0], n_calls = 10)\n",
    "print ('total time:', time.time() - start)\n",
    "print(res)\n",
    "print ('RES PRINT COMPLETE')\n",
    "fig = plt.figure()\n",
    "ax = plot_convergence(res)\n",
    "fig.axes.append(ax)\n",
    "plt.show()\n",
    "\n",
    "samp_num_list = np.array([args.samp_num, args.samp_num, args.samp_num, args.samp_num, args.samp_num])\n",
    "best_model = torch.load('./models/best_model.pt')\n",
    "best_model.eval()\n",
    "test_f1s = []\n",
    "batch_nodes = test_nodes\n",
    "adjs, input_nodes, output_nodes = default_sampler(np.random.randint(2**32 - 1), batch_nodes, samp_num_list * 20, len(feat_data), lap_matrix, args.n_layers)\n",
    "adjs = package_mxl(adjs, device)\n",
    "\n",
    "output = best_model.forward(feat_data[input_nodes], adjs)[output_nodes]\n",
    "test_f1s = [f1_score(output.argmax(dim=1).cpu(), labels[output_nodes].cpu(), average='micro')]\n",
    "print ('test f1 score')\n",
    "print (str(test_f1s))\n",
    "\n",
    "#print('Iteration: %d, Test F1: %.3f' % (oiter, np.average(test_f1s)))\n",
    "#s = ('Iteration: %d, Test F1: %.3f \\n' % (oiter, np.average(test_f1s)))\n",
    "#print (s)\n",
    "\n",
    "#print(feat_data.shape)\n",
    "#print(len(adjs))\n",
    "# print ('\\n\\nFINAL RESULT\\n')\n",
    "# print (str(res))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "i = 0\n",
    "# This function will be called by Bayesian optimization\n",
    "def ladies_iterate(x):\n",
    "    global cen\n",
    "    global i\n",
    "    print('iteration number', i)\n",
    "    i += 1\n",
    "\n",
    "    eigen_cen = cen[0]\n",
    "    bet_cen = cen[1]\n",
    "    clos_cen = cen[2]\n",
    "    deg_cen = cen[3]\n",
    "# Adding linearly to calculate probability\n",
    "    a = x[0]\n",
    "    b = x[1]\n",
    "    c = x[2]\n",
    "    d = x[3]\n",
    "    a = a / (a+b+c+d)\n",
    "    b = b / (a+b+c+d)\n",
    "    c = c / (a+b+c+d)\n",
    "    d = d / (a+b+c+d)\n",
    "    prob_unnorm = a * np.array(eigen_cen) + b * np.array(bet_cen) + c * np.array(clos_cen) + d * np.array(deg_cen)\n",
    "    prob_norm = prob_unnorm / np.linalg.norm( prob_unnorm, ord = 1 )\n",
    "\n",
    "    f.write('-----------------------------------------------------------------\\n')\n",
    "    print('-------------------------------------------------------------------')\n",
    "    strn = 'a = '+ str(a) + ', b = '+ str(b) + ', c = '+ str(c) + ', d = ' + str(d) + '\\n'\n",
    "    f.write(strn)\n",
    "    print('a = '+ str(a) + ' b = '+ str(b) + ' c = '+ str(c) + ', d = ' +  str(d) + '\\n')\n",
    "    process_ids = np.arange(args.batch_num)\n",
    "    samp_num_list = np.array([args.samp_num, args.samp_num, args.samp_num, args.samp_num, args.samp_num])\n",
    "\n",
    "    pool = mp.Pool(args.pool_num)\n",
    "    jobs = prepare_data(pool, sampler, process_ids, train_nodes, valid_nodes, samp_num_list, len(feat_data), lap_matrix, args.n_layers, prob_norm)\n",
    "    all_res = []\n",
    "    for oiter in range(1):\n",
    "        encoder = GCN(nfeat = feat_data.shape[1], nhid=args.nhid, layers=args.n_layers, dropout = 0.2).to(device)\n",
    "        susage  = SuGCN(encoder = encoder, num_classes=num_classes, dropout=0.5, inp = feat_data.shape[1])\n",
    "        susage.to(device)\n",
    "        \n",
    "        optimizer = optim.Adam(filter(lambda p : p.requires_grad, susage.parameters()))\n",
    "        best_val = 0\n",
    "        best_tst = -1\n",
    "        cnt = 0\n",
    "        times = []\n",
    "        res   = []\n",
    "        print('-' * 10)\n",
    "        for epoch in np.arange(args.epoch_num):\n",
    "            susage.train()\n",
    "            train_losses = []\n",
    "            train_data = [job.get() for job in jobs[:-1]]\n",
    "            valid_data = jobs[-1].get()\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            pool = mp.Pool(args.pool_num)\n",
    "            '''\n",
    "                Use CPU-GPU cooperation to reduce the overhead for sampling. (conduct sampling while training)\n",
    "            '''\n",
    "            jobs = prepare_data(pool, sampler, process_ids, train_nodes, valid_nodes, samp_num_list, len(feat_data), lap_matrix, args.n_layers, prob_norm)\n",
    "            for _iter in range(args.n_iters):\n",
    "                for adjs, input_nodes, output_nodes in train_data:    \n",
    "                    adjs = package_mxl(adjs, device)\n",
    "                    optimizer.zero_grad()\n",
    "                    t1 = time.time()\n",
    "                    susage.train()\n",
    "                    output = susage.forward(feat_data[input_nodes], adjs)\n",
    "                \n",
    "                    if args.sample_method == 'full':\n",
    "                        output = output[output_nodes]\n",
    "                    loss_train = F.cross_entropy(output, labels[output_nodes])\n",
    "                \n",
    "                    loss_train.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(susage.parameters(), 0.2)\n",
    "                    optimizer.step()\n",
    "                    times += [time.time() - t1]\n",
    "                    train_losses += [loss_train.detach().tolist()]\n",
    "                    del loss_train\n",
    "            susage.eval()\n",
    "            adjs, input_nodes, output_nodes = valid_data\n",
    "            adjs = package_mxl(adjs, device)\n",
    "            output = susage.forward(feat_data[input_nodes], adjs)\n",
    "            if args.sample_method == 'full':\n",
    "                output = output[output_nodes]\n",
    "            loss_valid = F.cross_entropy(output, labels[output_nodes]).detach().tolist()\n",
    "            valid_f1 = f1_score(output.argmax(dim=1).cpu(), labels[output_nodes].cpu(), average='micro')\n",
    "            print((\"Epoch: %d (%.1fs) Train Loss: %.2f    Valid Loss: %.2f Valid F1: %.3f\") %                   (epoch, np.sum(times), np.average(train_losses), loss_valid, valid_f1))\n",
    "            st = ('Epoch: %d (%.1fs) Train Loss: %.2f    Valid Loss: %.2f Valid F1: %.3f \\n') % (epoch, np.sum(times), np.average(train_losses), loss_valid, valid_f1) \n",
    "            f.write(st)\n",
    "\n",
    "            if valid_f1 > best_val + 1e-2:\n",
    "                best_val = valid_f1\n",
    "                torch.save(susage, './models/best_model.pt')\n",
    "                cnt = 0\n",
    "            else:\n",
    "                cnt += 1\n",
    "            if cnt == args.n_stops // args.batch_num:\n",
    "                break\n",
    "        \n",
    "    return (1-best_val)\n",
    "\n",
    "    '''\n",
    "            If using batch sampling for inference:\n",
    "    '''\n",
    "        #     for b in np.arange(len(test_nodes) // args.batch_size):\n",
    "        #         batch_nodes = test_nodes[b * args.batch_size : (b+1) * args.batch_size]\n",
    "        #         adjs, input_nodes, output_nodes = sampler(np.random.randint(2**32 - 1), batch_nodes,\n",
    "        #                                     samp_num_list * 20, len(feat_data), lap_matrix, args.n_layers)\n",
    "        #         adjs = package_mxl(adjs, device)\n",
    "        #         output = best_model.forward(feat_data[input_nodes], adjs)[output_nodes]\n",
    "        #         test_f1 = f1_score(output.argmax(dim=1).cpu(), labels[output_nodes].cpu(), average='micro')\n",
    "        #         test_f1s += [test_f1]\n",
    "        \n",
    "    '''\n",
    "            If using full-batch inference:\n",
    "    \n",
    "    '''\n",
    "#------------------------------------------------------\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.plots import plot_convergence\n",
    "\n",
    "start = time.time()\n",
    "res = gp_minimize(ladies_iterate,[(0.0, 1.0), (0.0, 1.0), (0.0, 1.0), (0.0, 1.0)], n_calls = 10)\n",
    "print ('total time:', time.time() - start)\n",
    "print(res)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plot_convergence(res)\n",
    "fig.axes.append(ax)\n",
    "plt.show()\n",
    "\n",
    "samp_num_list = np.array([args.samp_num, args.samp_num, args.samp_num, args.samp_num, args.samp_num])\n",
    "best_model = torch.load('./models/best_model.pt')\n",
    "best_model.eval()\n",
    "test_f1s = []\n",
    "batch_nodes = test_nodes\n",
    "adjs, input_nodes, output_nodes = default_sampler(np.random.randint(2**32 - 1), batch_nodes, samp_num_list * 20, len(feat_data), lap_matrix, args.n_layers)\n",
    "adjs = package_mxl(adjs, device)\n",
    "\n",
    "output = best_model.forward(feat_data[input_nodes], adjs)[output_nodes]\n",
    "test_f1s = [f1_score(output.argmax(dim=1).cpu(), labels[output_nodes].cpu(), average='micro')]\n",
    "print ('test f1 score')\n",
    "print (str(test_f1s))\n",
    "\n",
    "#print('Iteration: %d, Test F1: %.3f' % (oiter, np.average(test_f1s)))\n",
    "#s = ('Iteration: %d, Test F1: %.3f \\n' % (oiter, np.average(test_f1s)))\n",
    "#print (s)\n",
    "\n",
    "#print(feat_data.shape)\n",
    "#print(len(adjs))\n",
    "# print ('\\n\\nFINAL RESULT\\n')\n",
    "# print (str(res))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('lts': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "589776dd14f65bb17e24bc135d46bd968269cccd64c5cd0e2744d5b7f85103ad"
   }
  },
  "orig_nbformat": 2,
  "interpreter": {
   "hash": "589776dd14f65bb17e24bc135d46bd968269cccd64c5cd0e2744d5b7f85103ad"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}